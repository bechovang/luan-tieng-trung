#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Thesis Word Counter Tool
ƒê·∫øm s·ªë t·ª´ trong lu·∫≠n √°n ƒëa ng√¥n ng·ªØ (Trung-Vi·ªát-Anh)
"""

import re
import os
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import unicodedata
from collections import Counter
from datetime import datetime

class LatexParser:
    """X·ª≠ l√Ω v√† l√†m s·∫°ch n·ªôi dung LaTeX"""
    
    def __init__(self):
        # C√°c environment c·∫ßn lo·∫°i b·ªè
        self.ignore_environments = [
            'figure', 'table', 'equation', 'align', 'itemize', 'enumerate',
            'thebibliography', 'bibliography', 'abstract', 'quote', 'verse'
        ]
        
        # C√°c l·ªánh LaTeX c·∫ßn lo·∫°i b·ªè
        self.ignore_commands = [
            'chapter', 'section', 'subsection', 'subsubsection', 'paragraph',
            'footnote', 'footcite', 'cite', 'ref', 'label', 'caption',
            'includegraphics', 'url', 'href', 'textbf', 'textit', 'emph',
            'underline', 'textsuperscript', 'textsubscript', 'newline',
            'vspace', 'hspace', 'centering', 'raggedright', 'raggedleft',
            'makeatletter', 'makeatother', 'newcommand', 'renewcommand',
            'usepackage', 'documentclass', 'begin', 'end', 'input', 'include',
            'bibliographystyle', 'bibliography', 'graphicspath', 'frontmatter',
            'mainmatter', 'backmatter', 'tableofcontents', 'listoffigures',
            'listoftables', 'makecover', 'addcontentsline'
        ]
    
    def remove_comments(self, content: str) -> str:
        """Lo·∫°i b·ªè comments LaTeX"""
        # Lo·∫°i b·ªè comment m·ªôt d√≤ng
        content = re.sub(r'%.*$', '', content, flags=re.MULTILINE)
        return content
    
    def remove_latex_commands(self, content: str) -> str:
        """Lo·∫°i b·ªè c√°c l·ªánh LaTeX"""
        # Lo·∫°i b·ªè c√°c l·ªánh c√≥ tham s·ªë
        for cmd in self.ignore_commands:
            # Pattern cho l·ªánh c√≥ tham s·ªë trong {}
            pattern1 = rf'\\{cmd}\s*{{[^}}]*}}'
            content = re.sub(pattern1, '', content)
            
            # Pattern cho l·ªánh c√≥ tham s·ªë trong []
            pattern2 = rf'\\{cmd}\s*\[[^\]]*\]'
            content = re.sub(pattern2, '', content)
            
            # Pattern cho l·ªánh kh√¥ng c√≥ tham s·ªë
            pattern3 = rf'\\{cmd}\b'
            content = re.sub(pattern3, '', content)
        
        # Lo·∫°i b·ªè c√°c l·ªánh kh√°c c√≥ tham s·ªë
        content = re.sub(r'\\[a-zA-Z]+\s*{[^}]*}', '', content)
        content = re.sub(r'\\[a-zA-Z]+\s*\[[^\]]*\]', '', content)
        content = re.sub(r'\\[a-zA-Z]+\b', '', content)
        
        return content
    
    def remove_environments(self, content: str) -> str:
        """Lo·∫°i b·ªè c√°c environment LaTeX"""
        for env in self.ignore_environments:
            # Pattern cho environment v·ªõi n·ªôi dung
            pattern = rf'\\begin\s*{{{env}}}.*?\\end\s*{{{env}}}'
            content = re.sub(pattern, '', content, flags=re.DOTALL)
        
        # Lo·∫°i b·ªè c√°c environment kh√°c
        content = re.sub(r'\\begin\s*{[^}]*}.*?\\end\s*{[^}]*}', '', content, flags=re.DOTALL)
        
        return content
    
    def clean_content(self, content: str) -> str:
        """L√†m s·∫°ch to√†n b·ªô n·ªôi dung LaTeX"""
        # Lo·∫°i b·ªè comments tr∆∞·ªõc
        content = self.remove_comments(content)
        
        # Lo·∫°i b·ªè environments
        content = self.remove_environments(content)
        
        # Lo·∫°i b·ªè commands
        content = self.remove_latex_commands(content)
        
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát LaTeX
        content = re.sub(r'\\[{}]', '', content)  # Lo·∫°i b·ªè \{ v√† \}
        content = re.sub(r'\\[~^"]', ' ', content)  # Lo·∫°i b·ªè \~, \^, \"
        
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        content = re.sub(r'\s+', ' ', content)
        content = content.strip()
        
        return content

class LanguageDetector:
    """Ph√¢n lo·∫°i v√† ƒë·∫øm t·ª´ theo ng√¥n ng·ªØ"""
    
    def __init__(self):
        # Pattern cho ch·ªØ H√°n
        self.chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
        
        # Pattern cho ti·∫øng Vi·ªát (c√≥ d·∫•u) - c·∫£i thi·ªán
        self.vietnamese_pattern = re.compile(r'\b[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒëa-zA-Z]+\b')
        
        # Pattern cho ti·∫øng Anh v√† Latin
        self.english_pattern = re.compile(r'\b[a-zA-Z]+\b')
    
    def detect_chinese(self, text: str) -> List[str]:
        """T√¨m t·∫•t c·∫£ ch·ªØ H√°n"""
        return self.chinese_pattern.findall(text)
    
    def detect_vietnamese(self, text: str) -> List[str]:
        """T√¨m t·∫•t c·∫£ t·ª´ ti·∫øng Vi·ªát c√≥ d·∫•u"""
        return self.vietnamese_pattern.findall(text)
    
    def detect_english(self, text: str) -> List[str]:
        """T√¨m t·∫•t c·∫£ t·ª´ ti·∫øng Anh"""
        return self.english_pattern.findall(text)
    
    def count_by_language(self, text: str) -> Dict[str, int]:
        """ƒê·∫øm t·ª´ theo ng√¥n ng·ªØ"""
        chinese_words = self.detect_chinese(text)
        vietnamese_words = self.detect_vietnamese(text)
        english_words = self.detect_english(text)
        
        # Lo·∫°i b·ªè tr√πng l·∫∑p v√† t·ª´ r·ªóng
        chinese_count = len([w for w in set(chinese_words) if len(w.strip()) > 0])
        vietnamese_count = len([w for w in set(vietnamese_words) if len(w.strip()) > 0])
        english_count = len([w for w in set(english_words) if len(w.strip()) > 0])
        
        # T√≠nh t·ªïng th·ª±c t·∫ø (c√≥ th·ªÉ c√≥ overlap)
        total_unique = len(set(chinese_words + vietnamese_words + english_words))
        
        return {
            'chinese': chinese_count,
            'vietnamese': vietnamese_count,
            'english': english_count,
            'total': total_unique
        }

class ChapterStats:
    """Th·ªëng k√™ cho m·ªôt ch∆∞∆°ng"""
    
    def __init__(self, chapter_name: str, file_path: str):
        self.chapter_name = chapter_name
        self.file_path = file_path
        self.total_words = 0
        self.chinese_words = 0
        self.vietnamese_words = 0
        self.english_words = 0
        self.raw_content = ""
        self.clean_content = ""
        self.language_breakdown = {}
    
    def to_dict(self) -> Dict:
        """Chuy·ªÉn th√†nh dictionary"""
        return {
            'chapter_name': self.chapter_name,
            'file_path': self.file_path,
            'total_words': self.total_words,
            'chinese_words': self.chinese_words,
            'vietnamese_words': self.vietnamese_words,
            'english_words': self.english_words,
            'language_breakdown': self.language_breakdown
        }

class ThesisWordCounter:
    """Tool ch√≠nh ƒë·ªÉ ƒë·∫øm t·ª´ trong lu·∫≠n √°n"""
    
    def __init__(self, project_path: str = "."):
        self.project_path = Path(project_path)
        self.latex_parser = LatexParser()
        self.language_detector = LanguageDetector()
        self.chapters = []
        self.total_stats = {}
    
    def load_tex_files(self) -> List[Tuple[str, str]]:
        """ƒê·ªçc t·∫•t c·∫£ file .tex trong project"""
        tex_files = []
        processed_files = set()  # ƒê·ªÉ tr√°nh tr√πng l·∫∑p
        
        # ƒê·ªçc main.tex tr∆∞·ªõc
        main_tex = self.project_path / "main.tex"
        if main_tex.exists():
            with open(main_tex, 'r', encoding='utf-8') as f:
                tex_files.append(("main.tex", f.read()))
                processed_files.add("main.tex")
        
        # ƒê·ªçc c√°c file trong th∆∞ m·ª•c data
        data_dir = self.project_path / "data"
        if data_dir.exists():
            for file_path in data_dir.glob("*.tex"):
                if file_path.name not in processed_files:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        tex_files.append((file_path.name, f.read()))
                        processed_files.add(file_path.name)
        
        return tex_files
    
    def process_chapter(self, chapter_name: str, content: str, file_path: str) -> ChapterStats:
        """X·ª≠ l√Ω m·ªôt ch∆∞∆°ng"""
        stats = ChapterStats(chapter_name, file_path)
        
        # L√†m s·∫°ch n·ªôi dung LaTeX
        clean_content = self.latex_parser.clean_content(content)
        stats.clean_content = clean_content
        
        # ƒê·∫øm t·ª´ theo ng√¥n ng·ªØ
        language_counts = self.language_detector.count_by_language(clean_content)
        
        stats.total_words = language_counts['total']
        stats.chinese_words = language_counts['chinese']
        stats.vietnamese_words = language_counts['vietnamese']
        stats.english_words = language_counts['english']
        stats.language_breakdown = language_counts
        
        return stats
    
    def analyze_thesis(self) -> Dict:
        """Ph√¢n t√≠ch to√†n b·ªô lu·∫≠n √°n"""
        print("ƒêang ƒë·ªçc c√°c file .tex...")
        tex_files = self.load_tex_files()
        
        print(f"T√¨m th·∫•y {len(tex_files)} file .tex")
        
        # X·ª≠ l√Ω t·ª´ng file
        for file_name, content in tex_files:
            print(f"ƒêang x·ª≠ l√Ω: {file_name}")
            
            # X√°c ƒë·ªãnh t√™n ch∆∞∆°ng t·ª´ n·ªôi dung
            chapter_match = re.search(r'\\chapter\s*{([^}]+)}', content)
            if chapter_match:
                chapter_name = chapter_match.group(1)
            else:
                chapter_name = file_name.replace('.tex', '')
            
            stats = self.process_chapter(chapter_name, content, file_name)
            self.chapters.append(stats)
        
        # T√≠nh t·ªïng th·ªëng k√™
        self.calculate_total_stats()
        
        return self.generate_report()
    
    def calculate_total_stats(self):
        """T√≠nh t·ªïng th·ªëng k√™"""
        total_chinese = sum(chap.chinese_words for chap in self.chapters)
        total_vietnamese = sum(chap.vietnamese_words for chap in self.chapters)
        total_english = sum(chap.english_words for chap in self.chapters)
        total_words = sum(chap.total_words for chap in self.chapters)
        
        self.total_stats = {
            'total_words': total_words,
            'chinese_words': total_chinese,
            'vietnamese_words': total_vietnamese,
            'english_words': total_english,
            'num_chapters': len(self.chapters)
        }
    
    def generate_report(self) -> Dict:
        """T·∫°o b√°o c√°o t·ªïng h·ª£p"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_stats': self.total_stats,
            'chapters': [chap.to_dict() for chap in self.chapters],
            'summary': self.generate_summary()
        }
        
        return report
    
    def generate_summary(self) -> Dict:
        """T·∫°o t√≥m t·∫Øt"""
        if not self.chapters:
            return {}
        
        # T√¨m ch∆∞∆°ng d√†i nh·∫•t v√† ng·∫Øn nh·∫•t
        longest_chapter = max(self.chapters, key=lambda x: x.total_words)
        shortest_chapter = min(self.chapters, key=lambda x: x.total_words)
        
        # T√≠nh trung b√¨nh
        avg_words = self.total_stats['total_words'] / len(self.chapters)
        
        return {
            'longest_chapter': {
                'name': longest_chapter.chapter_name,
                'words': longest_chapter.total_words
            },
            'shortest_chapter': {
                'name': shortest_chapter.chapter_name,
                'words': shortest_chapter.total_words
            },
            'average_words_per_chapter': round(avg_words, 2),
            'language_distribution': {
                'chinese_percentage': round(self.total_stats['chinese_words'] / self.total_stats['total_words'] * 100, 2),
                'vietnamese_percentage': round(self.total_stats['vietnamese_words'] / self.total_stats['total_words'] * 100, 2),
                'english_percentage': round(self.total_stats['english_words'] / self.total_stats['total_words'] * 100, 2)
            }
        }
    
    def print_report(self, report: Dict):
        """In b√°o c√°o ra console"""
        print("\n" + "="*60)
        print("B√ÅO C√ÅO ƒê·∫æM T·ª™ LU·∫¨N √ÅN")
        print("="*60)
        
        # Th·ªëng k√™ t·ªïng quan
        total = report['total_stats']
        print(f"\nüìä TH·ªêNG K√ä T·ªîNG QUAN:")
        print(f"   T·ªïng s·ªë t·ª´: {total['total_words']:,}")
        print(f"   S·ªë ch∆∞∆°ng: {total['num_chapters']}")
        print(f"   - Ch·ªØ H√°n: {total['chinese_words']:,}")
        print(f"   - Ti·∫øng Vi·ªát: {total['vietnamese_words']:,}")
        print(f"   - Ti·∫øng Anh: {total['english_words']:,}")
        
        # Ph√¢n b·ªë ng√¥n ng·ªØ
        summary = report['summary']
        lang_dist = summary['language_distribution']
        print(f"\nüåê PH√ÇN B·ªê NG√îN NG·ªÆ:")
        print(f"   - Ch·ªØ H√°n: {lang_dist['chinese_percentage']}%")
        print(f"   - Ti·∫øng Vi·ªát: {lang_dist['vietnamese_percentage']}%")
        print(f"   - Ti·∫øng Anh: {lang_dist['english_percentage']}%")
        
        # Th·ªëng k√™ theo ch∆∞∆°ng
        print(f"\nüìñ TH·ªêNG K√ä THEO CH∆Ø∆†NG:")
        for chap in report['chapters']:
            print(f"   {chap['chapter_name']}: {chap['total_words']:,} t·ª´")
        
        # Ch∆∞∆°ng d√†i nh·∫•t v√† ng·∫Øn nh·∫•t
        longest = summary['longest_chapter']
        shortest = summary['shortest_chapter']
        print(f"\nüìà CHI TI·∫æT:")
        print(f"   Ch∆∞∆°ng d√†i nh·∫•t: {longest['name']} ({longest['words']:,} t·ª´)")
        print(f"   Ch∆∞∆°ng ng·∫Øn nh·∫•t: {shortest['name']} ({shortest['words']:,} t·ª´)")
        print(f"   Trung b√¨nh/ch∆∞∆°ng: {summary['average_words_per_chapter']:,} t·ª´")
        
        print("\n" + "="*60)
    
    def export_to_json(self, report: Dict, filename: str = "word_count_report.json"):
        """Export b√°o c√°o ra file JSON"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ ƒê√£ export b√°o c√°o ra file: {filename}")
    
    def export_to_excel(self, report: Dict, filename: str = "word_count_report.xlsx"):
        """Export b√°o c√°o ra file Excel"""
        try:
            # T·∫°o DataFrame cho c√°c ch∆∞∆°ng
            chapters_df = pd.DataFrame(report['chapters'])
            
            # T·∫°o DataFrame cho t·ªïng th·ªëng k√™
            total_df = pd.DataFrame([report['total_stats']])
            
            # T·∫°o DataFrame cho summary
            summary_df = pd.DataFrame([report['summary']])
            
            # Export ra Excel v·ªõi nhi·ªÅu sheet
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                chapters_df.to_excel(writer, sheet_name='Chapters', index=False)
                total_df.to_excel(writer, sheet_name='Total_Stats', index=False)
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
            
            print(f"‚úÖ ƒê√£ export b√°o c√°o ra file Excel: {filename}")
        except PermissionError:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o file Excel {filename} (file c√≥ th·ªÉ ƒëang ƒë∆∞·ª£c m·ªü)")
            print("   H√£y ƒë√≥ng file Excel n·∫øu ƒëang m·ªü v√† ch·∫°y l·∫°i")
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o file Excel: {e}")

def main():
    """H√†m ch√≠nh"""
    print("üîç B·∫Øt ƒë·∫ßu ƒë·∫øm t·ª´ lu·∫≠n √°n...")
    
    # Kh·ªüi t·∫°o counter
    counter = ThesisWordCounter()
    
    # Ph√¢n t√≠ch lu·∫≠n √°n
    report = counter.analyze_thesis()
    
    # In b√°o c√°o
    counter.print_report(report)
    
    # Export k·∫øt qu·∫£
    counter.export_to_json(report)
    counter.export_to_excel(report)
    
    print("\nüéâ Ho√†n th√†nh!")

if __name__ == "__main__":
    main() 